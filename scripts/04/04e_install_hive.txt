# Hive downloaden
# Man kann auch von Source installieren - siehe Anleitung unter https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-BuildingHivefromSource
# if you should need "--no-check-certificate" with wget, better do the following beforehand
sudo -s
update-ca-certificates -f
# Attention: Take care, that your hadoop version is compatible to the hive version - see https://hive.apache.org/general/downloads/
#            Unfortunately this is wrong, better use latest version 4.x, Hive 3.1.3 does not seem to work with our Hadoop version
#            The version below (4.0.1) was tested and works with Hadoop 3.3.6

cd /usr/local
export HIVE_VERSION=4.0.1
wget --no-check-certificate https://dlcdn.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz

tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz
ln -s /usr/local/apache-hive-${HIVE_VERSION}-bin /usr/local/hive

#########################################################

chown -R hduser:hadoop /usr/local/*hive*

# connect as hduser

su - hduser
cat >>~/.bashrc <<!
export HIVE_HOME=/usr/local/hive
export HCAT_HOME=\$HIVE_HOME/hcatalog
export PATH=\$PATH:\$HIVE_HOME/bin
# !!! optional, wenn java.long.OutOfMemoryError bei einzelnen Abfragen passiert, dann wäre folgendes zu setzen !!!
#export HADOOP_HEAPSIZE=2048
!

# damit Änderungen aktiv werden
source ~/.bashrc

# start HADOOP, if not running yet

start-dfs.sh
start-yarn.sh

# create directories for Hive
# if you see errors in $HADOOP_HOME/logs, if your Virtual disk is too small, in that case you should execute the following
#LC_ALL=C
#growpart /dev/sda1
#resize2fs /dev/sda1
#hdfs dfsadmin -safemode leave|enter

hdfs dfs -mkdir -p /tmp
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir -p /user/hduser/hive_external
# normalerweise reicht ein "g+w", aber wenn hive Connect als "anonymous" passiert, müssen Rechte a+w sein
hdfs dfs -chmod a+w /tmp
hdfs dfs -chmod a+w /user/hive/warehouse
hdfs dfs -chmod a+w /user/hduser/hive_external

# adapt hive-site.xml (take care to not copy a config file from a different version, as they might not be compatible)

cd /usr/local/hive/conf
cp hive-default.xml.template hive-site.xml

#Folgendes Property am Begin nach Tag <configuration> in hive-site.xml einfügen

 <property>
   <name>system:java.io.tmpdir</name>
   <value>/tmp/hive/java</value>
 </property>
 <property>
   <name>system:user.name</name>
   <value>${user.name}</value>
 </property>
 
# check for following tag and change it to same directory as used with the hdfs command above (take care, that hduser was write permissions)
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://namenode:9000/user/hive/warehouse</value>

# und im folgenden Wert muss eine absoluter Pfad für metastore_db angegeben werden, andernfalls wird immer eine neue DB angelegt, je nachdem, wo man beeline gerade startet
# (und man sieht die zuvor angelegten Tabellen  nicht mehr...)
# Der Wert sollte dann am besten nach dem Aufruf von "./schematool -initSchema -dbType derby" auf "create=false" gesetzt werden
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=/usr/local/hive/conf/metastore_db;create=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>


# und ganz wichtig: der folgende Wert muss ersetzt auf "false" gesetzt werden, bei "true" gibt es Exceptions bei join über 2 oder mehr Tabellen
  <property>
    <name>hive.auto.convert.join</name>
    <value>false</value>
    <description>Whether Hive enables the optimization about converting common join into mapjoin based on the input file size</description>
  </property>

# das folgende sicherheitshalber auch gleich setzen, das kann je nach Version zu Fehler:
# "user ... is not allowed to impersonate scott" führen 
  <property>   
     <name>hive.server2.enable.doAs</name>
     <value>false</value>
  </property>

### nachdem alle Einträge passiert sind, kann man hive mal testweise starten

hive
#

# bei folgendem Fehler passen die Versionen von hadoop und hive nicht zusammen, obwohl das auf der Downloadseite anders beschrieben ist:
# java.lang.NoSuchMethodError - siehe Fix unter https://issues.apache.org/jira/browse/HIVE-22915
mv $HIVE_HOME/lib/guava-19.0.jar /tmp
ln -s $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/

# die Warnings für log4j sollte man auch loswerden, indem man Folgendes verschiebt:
mv $HIVE_HOME/lib/log4j-slf4j-impl-2.18.0.jar /tmp

# Wenn es Fehler gibt, dann das hive-site.xml File nochmal prüfen, im default File gibt es bei gewissen Versionen folgenden Fehler, der händisch zu beheben ist (Sonderzeichen in Kommentar bei property hive.txn.xlock.iow)
#Caused by: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
# at [row,col,system-id]: [3221,96,"file:/usr/local/apache-hive-3.1.3-bin/conf/hive-site.xml"]

# Manchmal gehen erste Befehle gut und dann gibt es auf einmal Fehler in beeline.
# Fix siehe https://issues.apache.org/jira/browse/HIVE-21302 - dazu folgende 2 Einträge in hive-site.xml ändern:

<property>
   <name>datanucleus.schema.autoCreateAll</name>
  	<value>true</value>
   <description>creates necessary schema on a startup if one doesnt exist.</description>
</property>
<property>
   <name>hive.metastore.schema.verification</name>
   <value>false</value>
</property>

# damit hiveServer auf Console gleich loggen würde, wenn es einen Fehler gibt, folgendes machen
echo 'export HADOOP_CLIENT_OPTS="-Dhive.root.logger=console"' >$HIVE_HOME/conf/hive-env.sh

# init schema (das sollte das Verzeichnis metastore_db unter $HIVE_HOME/conf anlegen)
cd /usr/local/hive/bin

./schematool -initSchema -dbType derby
# wenn mal gar nichts mehr funktioniert und man neu beginnen will, am besten HDFS Verzeichnisse /user/hive/warehouse und /user/hduser/hive_external
# sowie $HIVE_HOME/conf/metastore_db rekursiv löschen

# Hive kann entweder lokal betrieben werden oder gegen Hive-Server. Ich hatte Probleme mit dieser Version im lokalen Betrieb, musste daher Hive-Server starten
# (andernfalls Exception: Connecting to jdbc:hive2:// java.lang.NoClassDefFoundError: org/apache/hive/service/cli/thrift/EmbeddedThriftBinaryCLIService
#        at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:176)
hive --service hiveserver2 &

# nach dem Starten des Hive-Servers, sollte der Status auch in folgender Web-GUI sichtbar sein (viel sieht man darin jedoch nicht)
"%ProgramFiles%\Google\Chrome\Application\chrome.exe" --new-tab http://localhost:10002
#firefox -new-tab http://localhost:10002

# Abhängig davon, ob man Hiveserver oder HiveEmbedded verwendet, ist unterschiedlicher connect-String zu verwenden (bei Embedded Mode ist die Variable leer)
export HIVE_CONNECT_STRING=localhost:10000

beeline --verbose
!connect jdbc:hive2://localhost:10000 scott tiger
# WICHTIG: Falls es Fehler "Connection refused" "user ... is not allowed to impersonate scott" bei beeline Kommando gibt,
# dann muss der Eintrag "hive.server2.enable.doAs" in hive-site.xml auf "false" gesetzt sein.

# Wenn obiges Prompt o.k. aussieht und nicht "closed" meldet, dann schaut Installation gut aus und man kann mal testweise Objekte anlegen 
beeline --verbose -u jdbc:hive2://$HIVE_CONNECT_STRING scott tiger <<!
   set hive.execution.engine=mr;
   set hive.metastore.warehouse.dir;
   show databases;
   use default;
   --
CREATE TABLE IF NOT EXISTS default.employee (
id int,
name string,
age int,
gender string )
COMMENT 'Employee Table'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
 
CREATE TABLE IF NOT EXISTS addresses (
cust_id STRING,
first_name STRING,
last_name STRING,
company_name STRING,
address STRING,
city STRING,
county STRING,
state STRING,
zip STRING,
phone1 STRING,
phone2 STRING,
email STRING,
web STRING )
COMMENT 'Adresses'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/tmp/addresses';
--
   show tables;
!
