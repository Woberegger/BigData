# Hive downloaden
# Man kann auch von Source installieren - siehe Anleitung unter https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-BuildingHivefromSource
# if you should need "--no-check-certificate" with wget, better do the following beforehand
sudo -s
update-ca-certificates -f
# Attention: Take care, that your hadoop version is compatible to the hive version - see https://hive.apache.org/general/downloads/
#            Unfortunately this is wrong, better use latest version 4.x, Hive 3.1.3 does not seem to work with our Hadoop version
#            The version 4.0.1 was tested and works with Hadoop 3.3.6, version 4.0.1 and 4.1.0 should work with hadoop 3.4.1

cd /usr/local
export HIVE_VERSION=4.0.1 # we better do not use 4.1.0, as this was compiled with Java17, so we would need an additional Java version for that, which differs to HDFS's java
# if the certificate is not accepted (although we have called the updata-ca-certificates), then add parameter: --no-check-certificate
wget https://archive.apache.org/dist/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz

tar -xzf apache-hive-${HIVE_VERSION}-bin.tar.gz
ln -s /usr/local/apache-hive-${HIVE_VERSION}-bin /usr/local/hive

#########################################################

chown -R hduser:hadoop /usr/local/*hive*

# connect as hduser

su - hduser
cat >>~/.bashrc <<!
export HIVE_HOME=/usr/local/hive
export HCAT_HOME=\$HIVE_HOME/hcatalog
export PATH=\$PATH:\$HIVE_HOME/bin
# um Fehler "java.long.OutOfMemoryError" vorzubeugen, das ist nämlich dann schwer zu finden, wo das passiert
export HADOOP_HEAPSIZE=2048
!

# damit Änderungen aktiv werden
source ~/.bashrc

# um sicherzugehen, aktualisieren wir die Daten aus dem Repo, weil wir später Dateien kopieren
cd ~/BigData
git pull

# start HADOOP, if not running yet

start-dfs.sh
start-yarn.sh

# create directories for Hive
# if you see errors in $HADOOP_HOME/logs, if your Virtual disk is too small, in that case you should execute the following
#LC_ALL=C
#growpart /dev/sda1
#resize2fs /dev/sda1
#hdfs dfsadmin -safemode leave|enter

hdfs dfs -mkdir -p /tmp
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir -p /user/hduser/hive_external
# normalerweise reicht ein "g+w", aber wenn hive Connect als "anonymous" passiert, müssen Rechte a+w sein
hdfs dfs -chmod a+w /tmp
hdfs dfs -chmod a+w /user/hive/warehouse
hdfs dfs -chmod a+w /user/hduser/hive_external

# adapt hive-site.xml (take care to not copy a config file from a different version, as they might not be compatible)

cd /usr/local/hive/conf
cp hive-default.xml.template hive-site.xml
# da in der Konfiguration sehr viele Einstellungen zu treffen sind und daher die Wahrscheinlichkeit eines Fehlers hoch, bitte das eingecheckte script nehmen!
cp ~/BigData/scripts/04/hive-site.xml .
# ACHTUNG !!! in dem Script sind jedoch username und passwort zu ändern, damit jeder sein eigenes Schema hat
HiveUserName=$(echo ${HOSTNAME//bigdata}hive)
sed -i "s/swd00hive/${HiveUserName}/g" hive-site.xml

# bei folgendem Fehler passen die Versionen von hadoop und hive nicht zusammen, obwohl das auf der Downloadseite anders beschrieben ist:
# java.lang.NoSuchMethodError - siehe Fix unter https://issues.apache.org/jira/browse/HIVE-22915
mv $HIVE_HOME/lib/guava-22.0.jar $HIVE_HOME/lib/guava-22.0.jar.wrong_version
ln -s $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/

# die Warnings für log4j sollte man auch loswerden, indem man Folgendes verschiebt:
mv $HIVE_HOME/lib/log4j-slf4j-impl-2.18.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.18.0.jar.wrong_version

# hive mal testweise starten
hive
#

# Wenn es Fehler gibt, dann das hive-site.xml File nochmal prüfen, im default File gibt es bei gewissen Versionen folgenden Fehler, der händisch zu beheben ist (Sonderzeichen in Kommentar bei property hive.txn.xlock.iow)
#Caused by: com.ctc.wstx.exc.WstxParsingException: Illegal character entity: expansion character (code 0x8
# at [row,col,system-id]: [3221,96,"file:/usr/local/apache-hive-3.1.3-bin/conf/hive-site.xml"]

# Manchmal gehen erste Befehle gut und dann gibt es auf einmal Fehler in beeline.
# Fix siehe https://issues.apache.org/jira/browse/HIVE-21302 - dazu folgende 2 Einträge in hive-site.xml ändern:

#<property>
#   <name>datanucleus.schema.autoCreateAll</name>
#  	<value>true</value>
#   <description>creates necessary schema on a startup if one doesnt exist.</description>
#</property>
#<property>
#   <name>hive.metastore.schema.verification</name>
#   <value>false</value>
#</property>

# weiter mit Script 04h_hive_metastore_mysql.txt bzw. bei Verwendung von lokalem Metastore mit local_mysql_docker/04f_install_mysql_docker.txt