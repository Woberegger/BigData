## Anleitung, um einen 2. oder 3. Knoten aufzusetzen, die als reine Datanodes, d.h. Rechenknechte, dienen.
# In virtualbox/VmWare am besten Snapshot erstellen und klonen (linked Clone, da kaum Speicher drauf geht) und dann am besten ein internes Netzwerk mit fixen IPs anlegen
# die Anleitung beschreibt das Aufsetzen eines 2. Knotens mit Hostnamen "UbuntuBigDataNode1"

# auf DataNode: wir müssen einen anderen hostnamen vergeben, ein Aufruf von "hostname" allein reicht nicht.
hostname UbuntuBigDataNode1
echo "UbuntuBigDataNode1" >/etc/hostname

# auf NameNode und Datanode:
sudo -s
cat >>/etc/hosts <<!
10.0.3.15 master namenode UbuntuBigData
10.0.3.16 UbuntuBigDataNode1 datanode1
!

# auf DataNode: normalerweise sollten die keys existieren, weil eben die Maschinen geklont wurden, andernfalls wechselseitig austauschen
ssh-keygen -t rsa -P ""
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@namenode
# einmalig einloggen, um Abfrage wegzubekommen
ssh hduser@namenode

# auf allen Nodes:
#### for cluster zusätzliche config in yarn-site.xml ("UbuntuBigData" ist der Hostname, den mit dem Wert des Ressource Manager Hosts ersetzen):
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>UbuntuBigData:8031</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>UbuntuBigData:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>UbuntuBigData:8032</value>
	</property>
## in hdfs-site.xml den Wert für dfs.replication auf 2 erhöhen

# auf Namenode: und dem master alle workers bekanntmachen
cat >$HADOOP_HOME/etc/hadoop/slaves <<!
UbuntuBigData
UbuntuBigDataNode1
!
echo "UbuntuBigData" >$HADOOP_HOME/etc/hadoop/masters
cp -p $HADOOP_HOME/etc/hadoop/slaves $HADOOP_HOME/etc/hadoop/workers

# auf allen Nodes:
# und damit Dateien doppelt abgelegt werden, sollte man in hdfs-site.xml die Variable dfs.replication auf Wert 2 setzen
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
        <property>
                <name>dfs.permissions</name>
                <value>false</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>
        </property>
</configuration>

### auf NameNode: Test call	

## NUR auf NameNode: Hadoop starten (als hduser) - auf den anderen Nodes sollte das autom. per pdsh mitgestartet werden.
start-dfs.sh
start-yarn.sh

# erwartete Ausgabe des "jps" Kommandos auf den datanodes
jps
#13076 DataNode --> wird über start-dfs.sh auf allen anderen Knoten mitgestartet
#13401 Jps
#13293 NodeManager --> wird über start-yarn.sh auf allen anderen Knoten mitgestartet


# das folgende erstellt HDFS-Verzeichnis und eine Datei   
hdfs dfs -mkdir -p /user/hduser/data
hdfs dfs -put <myTestfile> /user/hduser/data

# im Browser im Folgenden Pfad zu finden - bitte prüfen, ob der "Replication" Wert dem Wert 2 entspricht
firefox -new-tab http://localhost:9870/explorer.html#/user/hduser/data
