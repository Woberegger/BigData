## Anleitung am besten aus dem Buch "Big data in der Praxis" oder z.B. von https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-20-04 oder https://tecadmin.net/how-to-install-apache-hadoop-on-ubuntu-22-04/ --

##Packages Installieren
sudo -s
apt update
apt -y install pdsh
# tools, which make sense
apt -y install inetutils-telnet
apt -y install nmap
apt -y install curl
apt -y install wget
apt -y install vim
apt -y install vim-gtk3 # necessary on Debian image, where vim is built without clipboard support ("vim --version | grep clipboard")
apt -y install net-tools
apt -y install git
apt -y install gpg # needed for apt keys to add 

# important: in order that paste with mouse in vim works on Debian (for all users), you have to call the following - optionally change user's .vimrc
echo "set clipboard=unnamedplus" >>/etc/vim/vimrc.local # allow mouse-paste
echo "syntax on" >>/etc/vim/vimrc.local # enable coloured syntax highlighting

# on debian this is strange, we have to force to use the global settings in the user environments
su - debian -c "echo 'source /etc/vim/vimrc' >~debian/.vimrc"
echo 'source /etc/vim/vimrc' >/root/.vimrc

# we need to download older Java version from a different repository as the current one (as Debian "Trixie" only contains Java versions 21+, but hadoop 3.x.x only supports Java 8+11)
mkdir -m 0755 -p /etc/apt/keyrings/
wget -O - https://packages.adoptium.net/artifactory/api/gpg/key/public > /etc/apt/keyrings/adoption.gpg
echo "deb [signed-by=/etc/apt/keyrings/adoption.gpg] https://packages.adoptium.net/artifactory/deb trixie main" >/etc/apt/sources.list.d/adoptium.list

apt update # should find e.g. https://packages.adoptium.net/artifactory/deb package
export JAVA_FLAVOR=temurin-11-jdk
apt -y install $JAVA_FLAVOR

# openjdk-21-jdk is not compatible version with Hadoop 3.4.x
##Symbolischen Link für Java erstellen
### besser ist folgendes, das alle nötigen Pfade umsetzt:
# hier die Version 11 auswählen, falls mehr als 1 Version gefunden wird
update-alternatives --config java
update-alternatives --config javac
# optional dazu...
cd /usr/lib/jvm/
   ### abhängig von der Rechnerarchitektur könnte die Ausgabe von "dpkg --print-architecture" amd64 oder arm64 oder ??? liefern
   ### wenn dpkg Befehl nicht funktioniert, liefert auch "uname -m" Info zur Architektur, die jedoch dann interpretiert werden muss, da z.B. x86_64 als amd64 zu interpretieren ist
ln -sf ${JAVA_FLAVOR}-$(dpkg --print-architecture) jdk
# additionally it is good practice to verify, that /etc/alternatives/java points to correct java version
ls -lsa /etc/alternatives/java
echo "export JAVA_HOME=/usr/lib/jvm/${JAVA_FLAVOR}-$(dpkg --print-architecture)" >/etc/profile.d/java.sh
export SSH_PORT=22
echo "export SSH_PORT=$SSH_PORT" >>/etc/profile.d/java.sh
chmod 644 /etc/profile.d/java.sh
##
# erlaube Passwort-basierendes login für diese beiden user
cat >/etc/ssh/sshd_config.d/10-student.conf <<EOF
Match User student
   PasswordAuthentication yes
EOF

systemctl reload ssh || systemctl start ssh || service ssh start # Frage: Was macht das || in diesem Call?
# damit systemctl bei WSL funktioniert, muss "systemctl=true" in /etc/wsl.conf vorhanden sein - siehe auch 01b_prepare_wsl.txt

##Benutzer und Gruppe einrichten; als Passwort für hduser hadoop verwenden
groupadd hadoop
useradd -g hadoop -s /bin/bash -m hduser
echo "hduser:hadoop" | chpasswd # allows to change password by script
# add hduser to sudoers group
adduser hduser sudo

echo "$(hostname -I | cut -d' ' -f1) namenode" >>/etc/hosts
# die folgende 2 Zeilen wurden bereits beim Anlegen des images gemacht
#echo "10.77.17.48 datanode1" >>/etc/hosts # this is the common one
#echo "10.77.18.25 datanode2" >>/etc/hosts # this is the common one

# sinnvollerweise sollte man gleich das github Repo klonen, dann hat man alle Scripts, Sourcen etc. lokal auf der VM vorhanden
su - hduser
echo 'source /etc/vim/vimrc' >~/.vimrc

git clone https://github.com/Woberegger/BigData

##SSH Schlüssel erzeugen und hinzufügen
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub  > ~/.ssh/authorized_keys
# Schlüssel auf die beiden zusätzliche Datanodes kopieren
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@datanode1
ssh-copy-id -i ~/.ssh/id_rsa.pub hduser@datanode2

##SSH Schlüssel des Systems hinzufügen; mit yes bestätigen - sollte nach Passwort gefragt werden, dann funktionieren keys nicht
# (bzw. erlaubt die Option "-o" dies zu umgehen, was nur auf lokalem System zu empfehlen ist)
ssh -o StrictHostKeyChecking=accept-new localhost "ls -lsa"
# wenn das nicht funktioniert wegen Firewallregeln, dann entweder Windows-Firewall Regel anlegen lt. https://pureinfotech.com/open-port-firewall-windows-10/
# oder optional in WSL in /etc/ssh/sshd_config den Eintrag für "Port" z.B. wie folgt aktivieren auf einen Port > 1024 "Port=10222"
# danach in wsl als root "service ssh restart; service ssh status" ausführen und "ssh -p $SSH_PORT localhost" als "hduser" testen
# falls der Name anders aufgelöst wird, ist es nötig, auch mit dem Hostname und "namenode" einmalig die Verbindung zu öffnen
ssh -o StrictHostKeyChecking=accept-new $(hostname) "ls -lsa"
ssh -o StrictHostKeyChecking=accept-new namenode "ls -lsa"
## exit oder Ctrl<d> um wieder als ursprünglicher User zu arbeiten (da "hduser" nicht in sudoers enthalten ist)
exit

##Download Hadoop in passender Version und entpacken (in Browser herunterladen oder per wget/curl)
sudo -s
cd /usr/local
export HADOOP_VERSION=3.4.1
wget https://dlcdn.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz --no-check-certificate
wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz.sha512 --no-check-certificate
# das ShaSum File referenziert einen anderen Dateinamen, daher ist es zu ersetzen
# optional einfach "shasum -a 512 hadoop-${HADOOP_VERSION}.tar.gz" ausführen und mit Inhalt von *.sha512 Datei vergleichen
sed -i 's/-RC1//' hadoop-${HADOOP_VERSION}.tar.gz.sha512
shasum -a 512 hadoop-${HADOOP_VERSION}.tar.gz -c hadoop-${HADOOP_VERSION}.tar.gz.sha512

tar -xzf hadoop-${HADOOP_VERSION}.tar.gz
# oder gzip -d hadoop-${HADOOP_VERSION}.tar.gz && tar -xf hadoop-${HADOOP_VERSION}.tar
ln -sf hadoop-${HADOOP_VERSION} hadoop

chown -R hduser:hadoop hadoop*

##Hadoop konfigurieren
## Umgebungsvariablen in bashrc setzen

su - hduser
# WICHTIG: damit das mit geteilter VM für mehrere sekundäre Datanodes funktioniert, müssen wir den Namenode ident vorbereiten
export NAMENODEIP=$(hostname -I | cut -d' ' -f1)
ln -s /usr/local/hadoop/etc/hadoop /usr/local/hadoop/etc/datanode${NAMENODEIP}
export JAVA_FLAVOR=temurin-11-jdk
cat >>~/.bashrc <<EOF
# Java (a copy of what is already in /etc/profile.d/java.sh)
export JAVA_HOME=/usr/lib/jvm/${JAVA_FLAVOR}-$(dpkg --print-architecture)

# Hadoop export
export HADOOP_INSTALL=/usr/local/hadoop
export HADOOP_HOME=\$HADOOP_INSTALL
# specifically necessary because of shared datanodes
export HADOOP_CONF_DIR=\$HADOOP_INSTALL/etc/datanode${NAMENODEIP}
export HADOOP_MAPRED_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_HOME=\$HADOOP_INSTALL
export HADOOP_HDFS_HOME=\$HADOOP_INSTALL
export HADOOP_YARN_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=\$HADOOP_INSTALL/lib/native"
export YARN_HOME=\$HADOOP_HOME

export PATH=\$PATH:\$JAVA_HOME/bin:\$HADOOP_INSTALL/bin:\$HADOOP_INSTALL/sbin

export PDSH_RCMD_TYPE=ssh
EOF

##Speichern Sie die Änderungen in der Datei .bashrc. Da die bashrc nur beim erstellen der Konsole 
# gelesen wird, muss diese manuell neu eingelesen weerden. Verwenden Sie dazu den folgenden Befehl:

source ~/.bashrc

##Hadoop Konfigurationsdateien anpassen (als hduser)
# leider wird das auch in dieser Datei redundanterweise verlangt
echo "export JAVA_HOME=/usr/lib/jvm/${JAVA_FLAVOR}-$(dpkg --print-architecture)" >>${HADOOP_HOME}/etc/hadoop/hadoop-env.sh


##hier nehmen wir zur Sicherheit ein einheitliches Konfigurationsfile
cat >${HADOOP_CONF_DIR}/hdfs-site.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
   <property>
      <name>dfs.replication</name>
      <value>2</value>
   </property>
   <property>
      <name>dfs.namenode.maintenance.replication.min</name>
      <value>1</value>
   </property>
   <property>
      <name>dfs.namenode.hosts.provider.classname</name>
      <value>org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager</value>
   </property>
   <property>
      <name>dfs.hosts</name>
      <value>/usr/local/hadoop/etc/hadoop/hosts</value>
   </property>
   <property>
      <name>dfs.permissions</name>
      <value>false</value>
   </property>
   <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>
   </property>
   <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>
   </property>
   <property>
      <name>dfs.namenode.heartbeat.recheck-interval</name>
      <value>15000</value>
      <description>Determines datanode heartbeat interval in milliseconds</description>
   </property>
   <property>
      <name>dfs.block.size</name>
      <value>2097152</value>
   </property>
</configuration>
EOF

##Editieren Sie core-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
# um idente files bei Clusterverwendung zu haben, wird hier anstelle von "localhost" einen Namen lt. /etc/hosts eingetragen, hier eben immer "namenode"
cat >${HADOOP_CONF_DIR}/core-site.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://namenode:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop/hadoopdata/hdfs/tmp</value>
	</property>
</configuration>
EOF
   
##Editieren Sie mapred-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
cat >${HADOOP_CONF_DIR}/mapred-site.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.nodemanager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>
   <!-- IMPORTANT: set this parameter to use Yarn and not local mode for computation -->
   <property>
      <name>mapred.framework.name</name>
      <value>yarn</value>
   </property>
</configuration>
EOF

##Editieren Sie yarn-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
cat >${HADOOP_CONF_DIR}/yarn-site.xml <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.nodemanager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-check-enabled</name>
		<value>false</value>
	</property>
   <!-- IMPORTANT: set this parameter to use Yarn and not local mode for computation -->
   <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>namenode</value>
   </property>
</configuration>
EOF

##Erstellen Sie die für Hadoop benötigten Verzeichnisse (als User hduser)

mkdir -p /usr/local/hadoop/hadoopdata/hdfs/tmp
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/namenode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/datanode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/input

##Formatieren des HDFS File Systems (einmalig(!) auszuführen als User hduser)
hdfs namenode -format

##Hadoop starten (als hduser)
start-dfs.sh
start-yarn.sh

##oder über daemons (als hduser). Eher nur dann zum Debugging zu machen, falls ein Prozess nicht startet
#hdfs --daemon   start namenode
#hdfs --daemon   start datanode
#hdfs --daemon   start secondarynamenode
#yarn --daemon   start resourcemanager
#yarn --daemon   start nodemanager
#mapred --daemon start historyserver

##Checken ob Prozesse gestartet wurden

jps | sort -k2

##erwarteter Output (PID wird abweichen) sind folgende 6 Java-Prozesse
7444 DataNode
8129 Jps
7290 NameNode
7958 NodeManager
6024 ResourceManager
7642 SecondaryNameNode
# Bei Problem 'No such rcmd module "ssh"' kann es nötig sein, das Paket "pdsh-rcmd-ssh" zu installieren!

##Web-GUI starten (als der in der graphischen Oberfläche eingeloggte User, wegen $DISPLAY variable), bei WSL den Browser im Windows verwenden
# Namenode (nach start-dfs.sh) und Resourcemanager (nach start-yarn.sh)
# a) in Windows command prompt:
"%ProgramFiles%\Google\Chrome\Application\chrome.exe" --new-tab http://<ip_of_VM>:9870
"%ProgramFiles%\Google\Chrome\Application\chrome.exe" --new-tab http://<ip_of_VM>:8088
# b) unter Linux
firefox -new-tab http://<ip_of_VM>:9870/
firefox -new-tab http://<ip_of_VM>:8088/

# Bei Problemen in die Logfiles der Prozesse schauen (unter /usr/local/hadoop/logs)

##Hadoop stoppen (als hduser)
stop-yarn.sh 
stop-dfs.sh

##oder Hadoop daemons einzeln stoppen (als hduser)
#hdfs --daemon stop namenode
#hdfs --daemon stop datanode
#hdfs --daemon stop secondarynamenode
#yarn --daemon stop nodemanager
#yarn --daemon stop resourcemanager
#mapred --daemon stop historyserver
