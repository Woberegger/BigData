## Anleitung am besten aus dem Buch "Big data in der Praxis" oder z.B. von https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-20-04 oder https://tecadmin.net/how-to-install-apache-hadoop-on-ubuntu-22-04/ --

# Damit jeder individuelle Screenshots erstellen muss, bitte unique hostnamen erstellen - wichtig ist, dass Hostname mit Buchstaben beginnt
# Folgender Befehl nur in VM ausführen, in Wsl funktioniert dieser nicht
#sudo hostnamectl set-hostname swd<MatrikelNr>

##Packages Installieren
sudo apt-get update
sudo apt-get -y install ssh
sudo apt-get -y install pdsh
sudo apt-get -y install openjdk-11-jdk
sudo apt-get -y install openssh-server
# tools, which make sense
sudo apt-get -y install inetutils-telnet
sudo apt-get -y install nmap
sudo apt-get -y install curl
sudo apt-get -y install wget
sudo apt-get -y install vim
sudo apt-get -y install net-tools
sudo apt-get -y install git
# (Virtual-Box-only!!!) damit shared folders und copy'n'paste funktionieren, ist meist die Installation der Guest-Additions nötig, danach ev. ein Reboot der VM
#sudo apt-get install -y build-essential virtualbox-guest-utils virtualbox-guest-x11

sudo -s
##Symbolischen Link für Java erstellen
### besser ist folgendes, das alle nötigen Pfade umsetzt:
update-alternatives --config java
update-alternatives --config javac
# optional dazu...
cd /usr/lib/jvm/
   ### abhängig von der Rechnerarchitektur könnte die Ausgabe von "dpkg --print-architecture" amd64 oder arm64 oder ??? liefern
   ### wenn dpkg Befehl nicht funktioniert, liefert auch "uname -m" Info zur Architektur, die jedoch dann interpretiert werden muss, da z.B. x86_64 als amd64 zu interpretieren ist
ln -s java-11-openjdk-$(dpkg --print-architecture) jdk
# additionally it is good practice to verify, that /etc/alternatives/java points to correct java version
ls -lsa /etc/alternatives/java
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-$(dpkg --print-architecture)" >/etc/profile.d/java.sh
export SSH_PORT=22
echo "export SSH_PORT=$SSH_PORT" >>/etc/profile.d/java.sh
# prüfen, ob sshd läuft, andernfalls starten, je nach Plattform mit systemctl oder service command
# entweder man belässt den Port 22 oder man setzt besser einen nicht-privilegierten Port > 1024
# in /etc/ssh/sshd_config File (ich habe Port 10222 verwendet)
#sed -i 's/#Port 22/Port 10222/' /etc/ssh/sshd_config
systemctl start ssh || service ssh start # Frage: Was macht das || in diesem Call?
# damit systemctl bei WSL funktioniert, muss "systemctl=true" in /etc/wsl.conf vorhanden sein - siehe auch 01b_prepare_wsl.txt

# Wenn der gezeigte Port 22 ist, obwohl man einen anderen gesetzt hat (tritt z.B. bei Ubuntu 24-04 oder höher auf),
# bitte Anleitung 01c_different_ssh_port.txt beachten!

##Benutzer und Gruppe einrichten; als Passwort für hduser hduser verwenden
groupadd hadoop
useradd -g hadoop -s /bin/bash -m hduser 
passwd hduser

# da wir später einen Alias "namenode" verwenden über alle Clusterknoten, sollte man den bei wsl idealerweise
# in c:\windows\system32\drivers\etc\hosts eintragen, da dies andernfalls beim Neustart von wsl wieder gelöscht wird
# (optional den Parameter "generateHosts=false" in /etc/wsl.conf setzen - siehe Datei 01b_prepare_wsl.txt)
# der Alias kann in derselben Zeile wie "127.0.0.1 localhost" mit <BLANK> oder <TAB> angehängt oder in eigener Zeile stehen.
echo "127.0.0.1 namenode" >>/etc/hosts
# sinnvollerweise sollte man gleich das github Repo klonen, dann hat man alle Scripts, Sourcen etc. lokal auf der VM vorhanden
su - hduser
git clone https://github.com/Woberegger/BigData

##SSH Schlüssel erzeugen und hinzufügen
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub  > ~/.ssh/authorized_keys

##SSH Schlüssel des Systems hinzufügen; mit yes bestätigen - sollte nach Passwort gefragt werden, dann funktionieren keys nicht
# (bzw. erlaubt die Option "-o" dies zu umgehen, was nur auf lokalem System zu empfehlen ist)
ssh -p $SSH_PORT -o StrictHostKeyChecking=accept-new localhost "ls -lsa"
# wenn das nicht funktioniert wegen Firewallregeln, dann entweder Windows-Firewall Regel anlegen lt. https://pureinfotech.com/open-port-firewall-windows-10/
# oder optional in WSL in /etc/ssh/sshd_config den Eintrag für "Port" z.B. wie folgt aktivieren auf einen Port > 1024 "Port=10222"
# danach in wsl als root "service ssh restart; service ssh status" ausführen und "ssh -p $SSH_PORT localhost" als "hduser" testen
# falls der Name anders aufgelöst wird, ist es nötig, auch mit dem Hostname und "namenode" einmalig die Verbindung zu öffnen
ssh -p $SSH_PORT -o StrictHostKeyChecking=accept-new $(hostname) "ls -lsa"
ssh -p $SSH_PORT -o StrictHostKeyChecking=accept-new namenode "ls -lsa"
## exit oder Ctrl<d> um wieder als ursprünglicher User zu arbeiten (da "hduser" nicht in sudoers enthalten ist)
exit

##Download Hadoop in passender Version und entpacken (in Browser herunterladen oder per wget/curl)
sudo -s
cd /usr/local
export HADOOP_VERSION=3.3.6
wget https://dlcdn.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz --no-check-certificate
wget https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz.sha512 --no-check-certificate
# das ShaSum File referenziert einen anderen Dateinamen, daher ist es zu ersetzen
# optional einfach "shasum -a 512 hadoop-${HADOOP_VERSION}.tar.gz" ausführen und mit Inhalt von *.sha512 Datei vergleichen
sed -i 's/-RC1//' hadoop-${HADOOP_VERSION}.tar.gz.sha512
shasum -a 512 hadoop-${HADOOP_VERSION}.tar.gz -c hadoop-${HADOOP_VERSION}.tar.gz.sha512

tar -xzf hadoop-${HADOOP_VERSION}.tar.gz
# oder gzip -d hadoop-${HADOOP_VERSION}.tar.gz && tar -xf hadoop-${HADOOP_VERSION}.tar
ln -sf hadoop-${HADOOP_VERSION} hadoop

chown -R hduser:hadoop hadoop*

##Hadoop konfigurieren
## Umgebungsvariablen in bashrc setzen

su - hduser
cat >>~/.bashrc <<EOF
# Java (a copy of what is already in /etc/profile.d/java.sh)
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-$(dpkg --print-architecture)

# Hadoop export
export HADOOP_INSTALL=/usr/local/hadoop
export HADOOP_HOME=\$HADOOP_INSTALL
export HADOOP_MAPRED_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_HOME=\$HADOOP_INSTALL
export HADOOP_HDFS_HOME=\$HADOOP_INSTALL
export HADOOP_YARN_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=\$HADOOP_INSTALL/lib/native"
export YARN_HOME=\$HADOOP_HOME

export PATH=\$PATH:\$JAVA_HOME/bin:\$HADOOP_INSTALL/bin:\$HADOOP_INSTALL/sbin

export PDSH_RCMD_TYPE=ssh
EOF

##Speichern Sie die Änderungen in der Datei .bashrc. Da die bashrc nur beim erstellen der Konsole 
gelesen wird, muss diese manuell neu eingelesen weerden. Verwenden Sie dazu den folgenden Befehl:

source ~/.bashrc

##Hadoop Konfigurationsdateien anpassen (als hduser)
# leider wird das auch in dieser Datei redundanterweise verlangt
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-$(dpkg --print-architecture)" >>${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
echo "export HADOOP_SSH_OPTS=\"-p $SSH_PORT\"" >>${HADOOP_HOME}/etc/hadoop/hadoop-env.sh

##Editieren Sie core-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
# um idente files bei Clusterverwendung zu haben, besser anstelle von "localhost" einen Namen lt. /etc/hosts eintragen, z.B. "namenode"
# Damit dort beim ssh-Connect der einmalige Prompt nicht zu Fehler "Host key verification failed" kommt, muss man ausführen:
#ssh -p $SSH_PORT namenode

	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://namenode:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop/hadoopdata/hdfs/tmp</value>
	</property>


##Editieren Sie hdfs-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>
	</property>
   <property>
		<name>dfs.block.size</name>
		<value>4194304</value>
	</property>


##Editieren Sie mapred-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.nodemanager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>

##Editieren Sie yarn-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein

	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.nodemanager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-check-enabled</name>
		<value>false</value>
	</property>

##Erstellen Sie die für Hadoop benötigten Verzeichnisse (als User hduser)

mkdir -p /usr/local/hadoop/hadoopdata/hdfs/tmp
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/namenode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/datanode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/input

##Formatieren des HDFS File Systems (einmalig(!) auszuführen als User hduser)
hdfs namenode -format

##Hadoop starten (als hduser)
start-dfs.sh
start-yarn.sh

##oder über daemons (als hduser)
hdfs --daemon   start namenode
hdfs --daemon   start datanode
hdfs --daemon   start secondarynamenode
yarn --daemon   start resourcemanager
yarn --daemon   start nodemanager
mapred --daemon start historyserver

##Checken ob Prozesse gestartet wurden

jps | sort -k2

##erwarteter Output (PID wird abweichen) sind folgende 6 Java-Prozesse
7444 DataNode
8129 Jps
7290 NameNode
7958 NodeManager
6024 ResourceManager
7642 SecondaryNameNode
# Bei Problem 'No such rcmd module "ssh"' kann es nötig sein, das Paket "pdsh-rcmd-ssh" zu installieren!

##Web-GUI starten (als der in der graphischen Oberfläche eingeloggte User, wegen $DISPLAY variable), bei WSL den Browser im Windows verwenden
# Namenode (nach start-dfs.sh) und Resourcemanager (nach start-yarn.sh)
# a) in Windows command prompt:
"%ProgramFiles%\Google\Chrome\Application\chrome.exe" --new-tab http://localhost:9870
"%ProgramFiles%\Google\Chrome\Application\chrome.exe" --new-tab http://localhost:8088
# b) unter Linux
firefox -new-tab http://localhost:9870/
firefox -new-tab http://localhost:8088/

##Hadoop stoppen (als hduser)
stop-dfs.sh
stop-yarn.sh 

##oder Hadoop daemons einzeln stoppen (als hduser)
hdfs --daemon stop namenode
hdfs --daemon stop datanode
hdfs --daemon stop secondarynamenode
yarn --daemon stop nodemanager
yarn --daemon stop resourcemanager
mapred --daemon stop historyserver
