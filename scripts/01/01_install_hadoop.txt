## Anleitung am besten aus dem Buch "Big data in der Praxis" oder z.B. von https://www.digitalocean.com/community/tutorials/how-to-install-hadoop-in-stand-alone-mode-on-ubuntu-20-04 oder https://tecadmin.net/how-to-install-apache-hadoop-on-ubuntu-22-04/ --

# Damit jeder individuelle Screenshots erstellen muss, bitte unique hostnamen erstellen - der 2. Befehl ist nötig, damit dies auch nach Restart erhalten bleibt!
sudo -s
hostname <MatrikelNr>
echo "<MatrikelNr>" >/etc/hostname
echo "10.0.2.15 <MatrikelNr>" >>/etc/hosts

##Packages Installieren
sudo apt-get update
sudo apt-get install ssh
sudo apt-get install pdsh
sudo apt-get install openjdk-11-jdk
sudo apt-get install openssh-server
# tools, which make sense
sudo apt-get install nmap
sudo apt-get install curl
sudo apt-get install wget
sudo apt-get install vim
sudo apt-get install net-tools

##Symbolischen Link für Java erstellen
sudo -s
cd /usr/lib/jvm/
ln -s java-11-openjdk-amd64 jdk
# better than seting /usr/lib/jvm/jdk is to verify, that in /etc/alternatives points to correct java version
echo "export JAVA_HOME=/etc/alternatives/java" >/etc/profile.d/java.sh

##Benutzer und Gruppe einrichten; als Passwort für hduser hduser verwenden
groupadd hadoop
useradd -g hadoop -s /bin/bash -m hduser
passwd hduser

##SSH Schlüssel erzeugen und hinzufügen
su - hduser
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub  > ~/.ssh/authorized_keys

##SSH Schlüssel des Systems hinzufügen; mit yes bestätigen
ssh localhost

##2 mal Exit oder Ctrl<d> um wieder als ursprünglicher User zu arbeiten
exit
exit

##Download Hadoop 3.2.4 und entpacken (in Browser herunterladen oder per wget/curl)
sudo -s
cd /usr/local
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz --no-check-certificate
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz.sha512 --no-check-certificate
shasum -a 512 hadoop-3.2.4.tar.gz -c hadoop-3.2.4.tar.gz.sha512

tar -xzf hadoop-3.2.4.tar.gz
# oder gzip -d hadoop-3.2.4.tar.gz && tar -xf hadoop-3.2.4.tar
ln -sf hadoop-3.2.4 hadoop

chown -R hduser:hadoop hadoop*

##Hadoop konfigurieren
## Umgebungsvariablen in bashrc setzen

su - hduser
cat >>~hduser/.bashrc <<!
# Java (a copy of what is already in /etc/profile.d/java.sh)
export JAVA_HOME=/etc/alternatives/java # better than /usr/lib/jvm/jdk

# Hadoop export
export HADOOP_INSTALL=/usr/local/hadoop
export HADOOP_HOME=\$HADOOP_INSTALL
export HADOOP_MAPRED_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_HOME=\$HADOOP_INSTALL
export HADOOP_HDFS_HOME=\$HADOOP_INSTALL
export HADOOP_YARN_HOME=\$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=\$HADOOP_INSTALL/lib/native"
export YARN_HOME=\$HADOOP_HOME

export PATH=\$PATH:\$JAVA_HOME/bin:\$HADOOP_INSTALL/bin:\$HADOOP_INSTALL/sbin

export PDSH_RCMD_TYPE=ssh
!

##Speichern Sie die Änderungen in der Datei .bashrc. Da die bashrc nur beim erstellen der Konsole 
gelesen wird, muss diese manuell neu eingelesen weerden. Verwenden Sie dazu den folgenden Befehl:

source ~/.bashrc

##Hadoop Konfigurationsdateien anpassen (als hduser)
# leider wird das auch in dieser Datei redundanterweise verlangt
echo "export JAVA_HOME=/usr/lib/jvm/jdk" >>/usr/local/hadoop/etc/hadoop/hadoop-env.sh

##Editieren Sie core-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
# um idente files bei Clusterverwendung zu haben, besser anstelle von "localhost" einen Namen lt. /etc/hosts eintragen

	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://localhost:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop/hadoopdata/hdfs/tmp</value>
	</property>


##Editieren Sie hdfs-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/usr/local/hadoop/hadoopdata/hdfs/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/usr/local/hadoop/hadoopdata/hdfs/datanode</value>
	</property>


##Editieren Sie mapred-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein
	<property>
		<name>yarn.Node Manager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.Node Manager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.Node Manager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.Node Manager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>

##Editieren Sie yarn-site.xml in /usr/local/hadoop/etc/hadoop und fügen Sie zwischen den Tags <configuration> folgende Properties ein

	<property>
		<name>yarn.Node Manager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.Node Manager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.Node Manager.vmem-pmem-ratio</name>
		<value>3</value>
	</property>
	<property>
		<name>yarn.Node Manager.delete.debug-delay-sec</name>
		<value>600</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-check-enabled</name>
		<value>false</value>
	</property>

##Erstellen Sie die für Hadoop benötigten Verzeichnisse (als User hduser)

mkdir -p /usr/local/hadoop/hadoopdata/hdfs/tmp
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/namenode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/datanode
mkdir -p /usr/local/hadoop/hadoopdata/hdfs/input

##Formatieren des HDFS File Systems (einmalig(!) auszuführen als User hduser)
hdfs namenode -format

##Hadoop starten (als hduser)
start-dfs.sh
start-yarn.sh

##oder über daemons (als hduser)
hdfs --daemon   start namenode
hdfs --daemon   start datanode
hdfs --daemon   start secondarynamenode
yarn --daemon   start resourcemanager
yarn --daemon   start nodemanager
mapred --daemon start historyserver

##Checken ob Prozesse gestartet wurden

jps | sort -k2

##erwarteter Output (PID wird abweichen) sind folgende 6 Java-Prozesse
7444 DataNode
8129 Jps
7290 NameNode
7958 NodeManager
6024 ResourceManager
7642 SecondaryNameNode

##Web-GUI starten (als der eingeloggte User, wegen $DISPLAY variable)
# Namenode 
firefox -new-tab http://localhost:9870/
# ResourceManager 
firefox -new-tab http://localhost:8088/


##Hadoop stoppen (als hduser)
stop-dfs.sh
stop-yarn.sh 


##oder

##Hadoop stoppen (als hduser)
hdfs --daemon stop namenode
hdfs --daemon stop datanode
hdfs --daemon stop secondarynamenode
yarn --daemon stop nodemanager
yarn --daemon stop resourcemanager
mapred --daemon stop historyserver
