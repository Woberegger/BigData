# nach dem Installieren wollen wir evaluieren, wo und wie HDFS die Dateien ablegt
su - hduser
# eine gute Testdatei, die wir auch später für Mapreduce Beispiele verwenden, ist die folgende:
# entweder wie folgt direkt aus dem Internet runterladen, bzw. besser aus dem GitRepo 
# cd /tmp && wget https://github.com/DistrictDataLabs/transportation-project-1/raw/master/airdelayhist/data/airline_delay_causes.csv

# siehe Liste der Kommandos unter https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html
z.B.:hdfs dfs -ls # die letzten Kommandos mit '-' Prefix ähneln Linux OS-Kommandos

# optional sieht man Dateien/Ordner über "firefox -new-tab http://localhost:9870/explorer.html#/" (als der User, unter dem man in GUI eingeloggt ist)

# reingehängt haben wir das HDFS-Filesystem an folgendem Mountpoint
# mit folgendem Kommando zwischendurch prüfen, ob und auf welchem Knoten das nach einem "put" etc. sich verändert
du -d0 /usr/local/hadoop/hadoopdata

hdfs dfs -mkdir -p /user/hduser/testdir
hdfs dfs -ls -R /user/hduser/testdir # -R listet rekursiv
hdfs dfs -put -l ~/BigData/data/airline_delay_causes.csv /user/hduser/testdir/ # -l ... overruled die replications und speichert nur 1 Instanz

# versuchen, Trashbin zu aktivieren (prüfen, über welche Option) und dann verifizieren, ob die Datei wirklich dort landet
hdfs dfs -put ~/BigData/data/airline_delay_causes.csv /user/hduser/testdir/copy_to_delete.csv
hdfs dfs -rm -r /user/hduser/testdir/copy_to_delete.csv
hdfs dfs -rm -r -skipTrash /user/hduser/testdir/airline_delay_causes.csv
hdfs dfs -ls -R /user/hduser/.Trash/

# schauen, was passiert - werden auch die Replikas behalten im Trashbin?

# Herauf- und Heruntersetzen der Replikaanzahl "-setrep" call - was passiert? Prüfen über web GUI.
hdfs dfs -setrep -R 3 /user/hduser
hdfs dfs -setrep -R 2 /user/hduser

# Versuchen, die Blocksize bei einzelnen Files zu ändern - am besten eine recht große Datei verwenden, wo dann mehr als 2 Blöcke entstehen
hdfs dfs -D dfs.blocksize=1048576 -put ~/BigData/data/airline_delay_causes.csv /user/hduser/data/BlockSize1MB.csv

# Prüfen, über welchen Parameter man einstellen kann, ab wann auf einen toten DataNode reagiert wird. Ändern auf 60 Sekunden - siehe https://hadoop.apache.org/docs/r3.2.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
# Runterfahren des 2. Knotens und schauen, was nach 60 Sekunden mit den Replikas des Knotens passiert.
# Wenn man nur 1 Knoten betreibt, funktioniert das auch, man muss nur den DataNode Prozess killen über z.B.
kill $(jps | grep DataNode | cut -d' ' -f1)

# Timeout equals to 2 * heartbeat.recheck.interval + 10 * heartbeat.interval. Default for heartbeat.interval is 3 seconds, default for recheck-intervall is 300000
<property>
 <name>dfs.namenode.heartbeat.recheck-interval</name>
 <value>15000</value>
 <description>Determines datanode heartbeat interval in milliseconds</description>
</property>
# einen zuvor gekillten Datanode startet man danach am besten wieder direkt auf dem Datanode über:
hdfs --daemon start datanode

# "hdfs fsck" und "hdfs dfsadmin" Kommandos ausprobieren.
hdfs fsck  /user/hduser  -files -blocks -replicaDetails # hier sieht man, auf welchen Knoten die Repliken liegen, wieviele Blocks belegt werden usw.
#z.B. hdfs dfsadmin -allowSnapshot|-getDatanodeInfo <NodeName:Port>|-printTopology|-refreshNodes
# Sinn von Snapshots erheben und ausprobieren - was dauert lange und belegt Platz, das Anlegen oder erst das Ändern einer Datei?

# Zusatzaufgabe: Einstellen von Erasure Coding auf einem bestimmten Verzeichnis, nachdem dort einige sehr große Dateien abgelegt wurden. Was ändert sich am benötigten Space?
# !!! ACHTUNG: Dies erfordert jedoch 3 Nodes Minimum (in 3 verschiedenen Racks), dass dies funktioniert!!!
# siehe https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/RackAwareness.html
# das Script in $NF funktioniert nur, wenn jeder Datanode eine eigene IP-Adresse bekommt, daher besser random-Wert
cat >$HADOOP_HOME/bin/rack-topology.sh <<! 
#!/bin/bash
# Adjust/Add the property "net.topology.script.file.name"
# to core-site.xml with the "absolute" path to this
# file. ENSURE the file is "executable".
echo \$@ | xargs -n 1 | echo "/rack-"\$NF}'
!
chmod 755 $HADOOP_HOME/bin/rack-topology.sh

# Einfügen folgender Zeilen in core-site.xml
   <property>
      <name>net.topology.script.file.name</name>
      <value>/usr/local/hadoop/bin/rack-topology.sh</value>
   </property>
   
# Kopieren von core-site.xml und rack-topology.sh auf alle Knoten und Restart von hdfs
# Danach sollte es möglich sein, ein neues Verzeichnis anzulegen und dort die Policy zu ändern und zu überprüfen
hdfs dfsadmin -printTopology

hdfs ec -listPolicies # welche Policies gibt es, wähle diejenige aus, die mit den wenigsten Datanodes auskommt
hdfs ec -enablePolicy -policy <PolicyName>

hdfs dfs -mkdir /ErasureCoding
hdfs ec -setPolicy -path /ErasureCoding -policy <PolicyName>

# nachdem man eine grosse Datei reingestellt hat (1x in ein "normales" Verzeichnis und danach in ein ErasureCoding Verzeichnis, sollte man prüfen, wie die Größen anwachsen
du -d1 /usr/local/hadoop/hadoopdata

# Zusatzaufgabe: Was könnte folgende Fehlermeldung bedeuten - wann tritt sie auf, was kann man unternehmen?
hdfs dfs -put ~/BigData/data/airline_delay_causes.csv /user/hduser/data/

### put: Cannot create file/user/hduser/data/airline_delay_causes.csv._COPYING_. Name node is in safe mode. ###

# Zusatzaufgabe:
# suche Möglichkeit, hdfs-Filesysteme z.B. in NFS zu mounten (Anleitung unter https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html)
# oder per fuse über https://cwiki.apache.org/confluence/display/HADOOP2/MountableHDFS

# folgende Anleitung sollte funktionieren, ev. mit "systemctl status rpcbind" oder "rpcinfo -p $(hostname)" Status abfragen
# zuerst sind folgende Einträge in die Konfiguration einzutragen (möglichst offen für alle connects, um es einfach zu machen), danach dfs neu zu starten
a) in core-site.xml
	<property>
	  <name>hadoop.proxyuser.hduser.groups</name>
	  <value>*</value>
	</property>
	<property>
	  <name>hadoop.proxyuser.hduser.hosts</name>
	  <value>*</value>
	</property>
	<property>
	  <name>hadoop.proxyuser.root.groups</name>
	  <value>*</value>
	</property>
	<property>
	  <name>hadoop.proxyuser.root.hosts</name>
	  <value>*</value>
	</property>

b) in hdfs-site.xml:

	<property>
	  <name>dfs.namenode.accesstime.precision</name>
	  <value>3600000</value>
	</property>
	<property>
	  <name>dfs.nfs3.dump.dir</name>
	  <value>/tmp/.hdfs-nfs</value>
	</property>
	<property>
	  <name>dfs.nfs.exports.allowed.hosts</name>
	  <value>* rw</value>
	</property>
	<property>
	  <name>nfs.metrics.percentiles.intervals</name>
	  <value>100</value>
	</property>
	<property>
	  <name>nfs.port.monitoring.disabled</name>
	  <value>false</value>
	</property>

sudo apt install rpcbind
sudo systemctl start rpcbind # sudo /etc/init.d/rpcbind start # (in WSL)
sudo apt install nfs-common
sudo /etc/init.d/nfs-common start #(nötig in WSL)
su - hduser
   stop-dfs.sh
   start-dfs.sh
   hdfs --daemon start nfs3 # oder im Vordergrund über "hdfs nfs3" (vor allem, wenn folgendes jps Kommando keinen laufenden Prozess findet, sieht man hier die Fehlerausgabe)
   jps | grep Nfs3 

sudo mkdir /mnt/hdfs
sudo mount -t nfs -o vers=3,proto=tcp,nolock,noacl,sync localhost:/ /mnt/hdfs

# dann sollte man Inhalte des HDFS im normalen Filesystem sehen
ls -lsa /mnt/hdfs/user/hduser/testdir


