# Die Aufgaben in diesem File sind mit 1 aktiven Datanode möglich, wobei man mehr mitkriegt, wie HDFS funktioniert,
# wenn mehr als 1 (idealerweise 3) Datanodes angelegt wurden. Daher, falls genug Zeit vorhanden ist,
# vorher mit einer der Anleitungen weitere Datanodes aktivieren
### WICHTIG: alle Kommandos unter user "hduser" ausführen ###
su - hduser
# Wenn man weitere Datanodes ins "workers" File dazugehängt hat, dann kann man entweder stop-dfs.sh und start-dfs.sh ausführen
# oder man macht das, was produktiv sinnvoller ist und hängt die Datenodes zur Laufzeit dazu
hadoop-daemons.sh start datanode --hosts datanode1
hadoop-daemons.sh start datanode --hosts datanode2

# nach dem Installieren und Konfigurieren wollen wir evaluieren, wo und wie HDFS die Dateien ablegt

# eine gute Testdatei, die wir auch später für Mapreduce Beispiele verwenden, ist die folgende:
# entweder wie folgt direkt aus dem Internet runterladen, bzw. besser aus dem GitRepo ~/BigData/data/airline_delay_causes.csv
# cd /tmp && wget https://github.com/DistrictDataLabs/transportation-project-1/raw/master/airdelayhist/data/airline_delay_causes.csv

# siehe Liste der Kommandos unter https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html
z.B.:hdfs dfs -ls # die letzten Kommandos mit '-' Prefix ähneln Linux OS-Kommandos

# optional sieht man Dateien/Ordner über "firefox -new-tab http://namenode:9870/explorer.html#/" (als der User, unter dem man in GUI eingeloggt ist)

# reingehängt haben wir das HDFS-Filesystem an folgendem Mountpoint
# mit folgendem Kommando zwischendurch prüfen, ob und auf welchem Knoten das nach einem "put" etc. sich verändert
du -d0 /usr/local/hadoop/hadoopdata

hdfs dfs -mkdir -p /user/hduser/testdir
hdfs dfs -ls -R /user/hduser/testdir # -R listet rekursiv
hdfs dfs -put -l ~/BigData/data/airline_delay_causes.csv /user/hduser/testdir/ # -l ... overruled die replications und speichert nur 1 Instanz

# versuchen, Trashbin zu aktivieren (prüfen, über welche Option) und dann verifizieren, ob die Datei wirklich dort landet
hdfs dfs -put ~/BigData/data/airline_delay_causes.csv /user/hduser/testdir/copy_to_delete.csv
hdfs dfs -rm -r /user/hduser/testdir/copy_to_delete.csv
hdfs dfs -rm -r -skipTrash /user/hduser/testdir/airline_delay_causes.csv
hdfs dfs -ls -R /user/hduser/.Trash/

# schauen, was passiert - werden auch die Replikas behalten im Trashbin?

# Herauf- und Heruntersetzen der Replikaanzahl "-setrep" call - was passiert? Prüfen über web GUI.
hdfs dfs -setrep -R 3 /user/hduser
hdfs dfs -setrep -R 2 /user/hduser

# Versuchen, die Blocksize bei einzelnen Files zu ändern - am besten eine recht große Datei verwenden, wo dann mehr als 2 Blöcke entstehen
hdfs dfs -D dfs.blocksize=1048576 -put ~/BigData/data/airline_delay_causes.csv /user/hduser/testdir/BlockSize1MB.csv

# Prüfen, über welchen Parameter man einstellen kann, ab wann auf einen toten DataNode reagiert wird. Ändern auf 60 Sekunden - siehe https://hadoop.apache.org/docs/r3.2.4/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
# Runterfahren des zugeteilten Prozesses des 2. Knotens und schauen, was nach 60 Sekunden mit den Replikas des Knotens passiert.
# Wann werden Replikas von einem Knoten auf den anderen übertragen, damit wir wieder die gewünschte Anzahl von 2 haben?
# Da wir auf den weiteren Nodes ja alle gemeinsam arbeiten, muss man durch Übergabe der IP des Namenodes an das grep-Kommando die richtige Instanz finden
kill $(jps -v | grep datanode<IP-Address> | cut -d' ' -f1)

# Timeout equals to 2 * heartbeat.recheck.interval + 10 * heartbeat.interval. Default for heartbeat.interval is 3 seconds, default for recheck-intervall is 300
#        <property>
#                <name>dfs.namenode.heartbeat.recheck-interval</name>
#                <value>60</value>
#        </property>
#        <property>
#                <name>dfs.namenode.stale.datanode.interval</name>
#                <value>10</value>
#        </property>
# einen zuvor gekillten Datanode startet man danach am besten wieder direkt auf dem Datanode über (optional auf namenode über "start-dfs.sh" mit Warnings):
export SSH_CLIENT=<NameNode-IP-Address> # z.B. 10.77.16.124 (damit wir sicher unseren eigenen Datanode neu starten)
source ~/datanode.env
hdfs --daemon start datanode

# "hdfs fsck" und "hdfs dfsadmin" Kommandos ausprobieren.
hdfs fsck  /user/hduser  -files -blocks -replicaDetails # hier sieht man, auf welchen Knoten die Repliken liegen, wieviele Blocks belegt werden usw.
#z.B. hdfs dfsadmin -allowSnapshot|-getDatanodeInfo <NodeName:Port>|-printTopology|-refreshNodes
# Sinn von Snapshots erheben und ausprobieren - was dauert lange und belegt Platz, das Anlegen oder erst das Ändern einer Datei?

# Zusatzaufgabe: Was könnte folgende Fehlermeldung bedeuten - wann tritt sie auf, was kann man unternehmen?
hdfs dfs -put ~/BigData/data/airline_delay_causes.csv /user/hduser/data/

### put: Cannot create file/user/hduser/data/airline_delay_causes.csv._COPYING_. Name node is in safe mode. ###

# optionale Zusatzaufgabe (für Zusatzpunkte) siehe Datei 02c_erasure_encoding.txt

# optionale Zusatzaufgabe (für Zusatzpunkte) siehe Datei 02d_nfs_mount.txt