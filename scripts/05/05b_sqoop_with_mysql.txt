# und damit man mit Sqoop auf Datenbanken zugreifen kann, braucht man z.B. einen entspr. JDBC Connector
# fuer mySQL
# VORSICHT: MySQL-Objekte sind per default case-sensitive, d.h. studentsMySQL != studentsmysql

su - hduser
cd $SQOOP_HOME
# fuer ubuntu könnte man auch ein Debian-Package verwenden, aber wir bleiben bei den generischen tar-Files:
# alternativ download unter http://www.mysql.com/downloads/connector/j/5.1.html
MYSQLCON_VERSION=8.0.29
wget http://ftp.ntu.edu.tw/MySQL/Downloads/Connector-J/mysql-connector-java-${MYSQLCON_VERSION}.tar.gz
# wir müssen nur das jar-File verwenden und ins lib-Directory kopieren
tar -xzf mysql-connector-java-${MYSQLCON_VERSION}.tar.gz
cp -p mysql-connector-java-${MYSQLCON_VERSION}/mysql-connector-java-${MYSQLCON_VERSION}.jar lib/

# und dann halt mySQL selber (entweder direkt oder als docker container) - als root/osboxes-user (z.B. in anderer shell)
sudo apt install mysql-server
sudo systemctl start mysql.service

sudo mysql <<!
   ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'my-secret-pw';
!

su - hduser
# Wichtig der MySQL user braucht ein Verzeichnis im hdfs, anderenfalls gibt es exception "java.io.IOException: java.lang.ClassNotFoundException: students"
export MYSQL_USER=root
hdfs dfs -mkdir /user/${MYSQL_USER}
hdfs dfs -chmod 777 /user/${MYSQL_USER}

export DATABASE_NAME=swd
# relogin mit dem gesetzten Passwort:
mysql -u $MYSQL_USER -p <<!

CREATE DATABASE IF NOT EXISTS $DATABASE_NAME;
show databases;
use $DATABASE_NAME;

CREATE TABLE IF NOT EXISTS studentsMySQL (
    id INT PRIMARY KEY,
    first_name VARCHAR(80) NOT NULL,
    last_name VARCHAR(80) NOT NULL,
    entry_date DATE,
    course VARCHAR(80) NOT NULL
);
show tables;
!

# jetzt legen wir eine idente Tabelle in Hive an
# wenn Hive-Server (hive --service hiveserver2 &) verwendet wird, dann muss der Connectstring so gesetzt sein, andernfalls leer
export HIVE_CONNECT_STRING=localhost:10000
beeline -u jdbc:hive2://${HIVE_CONNECT_STRING} scott tiger <<!
   set hive.execution.engine=mr;
   set hive.metastore.warehouse.dir;
   create database if not exists fh;
   show databases;
   use fh;
   show tables;
   -- diese Tabelle kopieren wir von Hive nach mySQL
   DROP table students;
   CREATE TABLE IF NOT EXISTS students( id INT , first_name STRING, last_name STRING, entry_date DATE, course STRING)
      COMMENT 'Students to sync with mySQL'
      ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
   describe students;
   -- und in diese Tabelle wieder retour von MySQL
   DROP table studentsFromMySQL;
   CREATE TABLE IF NOT EXISTS studentsFromMySQL ( id INT , first_name STRING, last_name STRING, entry_date DATE, course STRING)
   COMMENT 'Students to sync with mySQL';
!

# dann generiere ein csv-File über https://generatedata.com/ mit den benötigten Daten.
# Eine bereits generierte Version liegt im git Repo unter "data/students.csv"
# die generierten Daten ev. noch nachbearbeiten:
# a) Split auf Vorname+Nachname:
#    sed -i 's/ /,/g' students.csv
# b) ev. passendes Datumsformat mit padding (ist aber nicht nötig für unseren Testfall):

# wir können die Datei ins hdfs reinladen oder benutzen hier mal das "LOCAL INPATH" zum Laden von lokaler Datei
# hdfs dfs -put BigData/data/students.csv /tmp/
beeline -u jdbc:hive2://${HIVE_CONNECT_STRING} scott tiger <<!
   use fh;
   LOAD DATA LOCAL INPATH '/home/hduser/BigData/data/students.csv' OVERWRITE INTO TABLE students;
   select * from students limit 10;
!

# scheinbar muss man die mapreduce Jar Files rüberkopieren, andernfalls gibt es "ClassNotFound" exception
cp -p /usr/local/hadoop/share/hadoop/mapreduce/*.jar $SQOOP_HOME/lib/
# weiters generiert Sqoop eine Klasse "students.jar" für den Import, die im ClassPath gefunden werden muss
# Daher ist der Parameter --bindir auf z.B. /usr/local/hadoop/share/hadoop/common/lib zu setzen

export DATABASE_NAME=swd
# Wichtig: port forward für mysql docker container auf 13306, native auf 3306
export MYSQL_PORT=3306
export MYSQL_PASSWD=my-secret-pw
# liste auf, ob unsere Datenbank und unsere Tabelle "studentsMySQL" gefunden werden
sqoop-list-databases --connect jdbc:mysql://localhost:${MYSQL_PORT}  --username $MYSQL_USER --password ${MYSQL_PASSWD}
sqoop-list-tables --connect jdbc:mysql://localhost:${MYSQL_PORT}/${DATABASE_NAME} --username $MYSQL_USER --password ${MYSQL_PASSWD}

# -m 1 ... Anzahl an Mappern, hier derzeit immer 1
sqoop export --connect jdbc:mysql://localhost:${MYSQL_PORT}/${DATABASE_NAME}  \
   --table studentsMySQL --username $MYSQL_USER --password ${MYSQL_PASSWD}  \
   --export-dir /user/hive/warehouse/fh.db/students --num-mappers 1  \
   --driver com.mysql.cj.jdbc.Driver  --input-fields-terminated-by ',' \
   --input-lines-terminated-by '\n' --bindir /usr/local/hadoop/share/hadoop/common/lib

mysql -u $MYSQL_USER -p <<!
use $DATABASE_NAME;
SELECT * FROM studentsMySQL LIMIT 10;
!

# die andere Richtung am besten testen, indem eine 2. Tabelle in Hive angelegt wird
# Verzeichnis löschen, falls von vorigem Versuch noch was existiert
hdfs dfs -rm -R /user/hduser/studentsMySQL 2>/dev/null
# möglicherweise gibt es Fehler, der jedoch üblicherweise eine Folgeerscheinung ist, weil die Java VM den Zugriff auf die Datei
# /usr/local/hive/lib/derby-10.14.2.0.jar nicht erlaubt:
#java.lang.NoClassDefFoundError: Could not initialize class org.apache.derby.jdbc.EmbeddedDriver
# Daher Inhalt von File ../grp02/grp02_java_policy_to_add.txt am Beginn von Datei /etc/java-11-openjdk/security/java.policy einzutragen:
https://db.apache.org/derby/docs/10.13/security/rsecpolicysample.html
bzw. folgende Zeile in den generischen grant { ... } Abschnitt:
    permission javax.management.MBeanTrustPermission "register";

sqoop import --connect jdbc:mysql://localhost:${MYSQL_PORT}/${DATABASE_NAME}  --username $MYSQL_USER --password ${MYSQL_PASSWD} \
   --table studentsMySQL  --driver com.mysql.cj.jdbc.Driver  --hive-import --hive-database fh \
   --hive-table studentsFromMySQL --hive-overwrite --num-mappers 1  \
   --bindir /usr/local/hadoop/share/hadoop/common/lib

# Sollte obiger Befehl nicht funktionieren, kann man versuchen, die MySQL Daten zumindest ins HDFS abzuholen und nicht direkt in Hive reinschreiben.
sqoop import --connect jdbc:mysql://localhost:${MYSQL_PORT}/${DATABASE_NAME}  --username $MYSQL_USER --password ${MYSQL_PASSWD} \
   --table studentsMySQL  --driver com.mysql.cj.jdbc.Driver  --target-dir /user/hduser/studentsMySQL

# verifiziere die Inhalte
beeline -u jdbc:hive2://${HIVE_CONNECT_STRING} scott tiger <<!
   set hive.execution.engine=mr;
   set hive.metastore.warehouse.dir;
   use fh;
   SELECT * FROM studentsFromMySQL LIMIT 10;
!